\documentclass[../notes.tex]{subfiles}

\begin{document}

\section{Outlier Detection}
\subsection{Introduction}
\subsubsection{Overview}
\begin{itemize}
  \item Clustering discovers groups of similar objects, while outlier detection seeks individual objects that are different from all clusters.
  \item Most outlier detection methods create a model of normal patterns. Outliers are the objects that do not fit within this normal model.
  \item Outlier detection is unsupervised, i.e. we have no examples of outliers.
  \item Outliers are also referred to as anomalies, abnormalities, discordant, deviants.
  \item Applications of outlier detection
  \begin{itemize}
    \item Data cleaning: remove outliers.
    \item Fraud detection: abnormal behavior may indicate fraud.
    \item Network intrusion detection: abnormal traffic may indicate intrusion.
  \end{itemize}
\end{itemize}

\subsubsection{Types of Outliers}
\begin{itemize}
  \item Point outliers
  \begin{itemize}
    \item An individual object that deviates significantly from the rest of the dataset.
  \end{itemize}

  \item Contextual outliers
  \begin{itemize}
    \item An individual object that deviates significantly from its context within the dataset.
  \end{itemize}

  \item Collective outliers
  \begin{itemize}
    \item A set of related objects that deviates significantly from the dataset.
  \end{itemize}

\end{itemize}

\subsection{Extreme value analysis}
\subsubsection{Method}
\begin{itemize}
  \item Assume that all data has been generated from a probability distribution of known type, e.g. Gaussian distribution
  $$P(x) = \frac{1}{\sqrt{(2 \pi)^d |\Sigma|}} e^{\frac{1}{2}(x-\mu)^T \cdot \Sigma^{-1} \cdot (x-\mu)}$$
  \item Parameters of the distribution known from domain knowledge or estimated from data
  \item Outlier: object within tail of probability distribution
  \item Z-number as outlier score $z = \frac{x - \mu}{\sigma}$ 
  \begin{itemize}
    \item If the absolute value of the Z-number is greater than 3, the object is considered as an extreme value (outlier).
  \end{itemize}   
\end{itemize}

\subsubsection{Histogram-based Method}
\begin{itemize}
  \item Non-parametric, i.e. does not assume a given probability distribution
  \item Outlier: object that has less than $t\%$ other objects with more extreme values
  \item Disadvantage: Hard to choose an appropriate bin size for histogram
\end{itemize}

\newpage

\subsection{Probabilistic methods}
\begin{itemize}
  \item Generalizes extreme value analysis.
  \item Assume that data has been generated from a mixture of multiple probability distributions.
  \item Often, assume a mixture of $k$ Gaussians:
  $$P(x) = \sum_{i=1}^k W_i \cdot P(x|C_i)$$
  \item Outliers are defined as those objects that are highly unlikely to be generated by this model.
\end{itemize}

\subsection{Clustering-based methods}
\begin{itemize}
  \item Assumption: normal objects belong to large clusters, while outliers do not belong to any of the clusters or form very small clusters
  
  \item Density-based clustering
  \begin{itemize}
    \item Outliers are not included in any cluster
    \item Outlier score depends on density in neighborhood
  \end{itemize}
  
  \item Distance-based clustering
  \begin{itemize}
    \item Outlier are objects with largest distances from cluster representative.
    \item Outlier score depends on distance from nearest cluster representative.
  \end{itemize}

  \item Result depends on chosen clustering algorithm.
\end{itemize}

\subsection{Distance-based methods}
\begin{itemize}
  \item Outliers data are far away from the ``crowed regions''

  \item Approach 1
  \begin{itemize}
    \item Count percentage of objects within given distance $r$, the fewer objects, the higher the outlier score.
    \item $\frac{||\{o' | dist(o', o) \le r\}||}{||D||} \le \pi$
  \end{itemize}
   
  \item Approach 2: 
  \begin{itemize}
    \item compute distance to k-th-nearest neighbor, the higher the distance,the higher the outlier score.
    \item knn-dist($o$) $\ge \omega$
    \item knn-dist($o$) is the distance from $o$ to its kth-nearest neighbor. There can be multiple objects that have knn-dist($o$) from $o$. 
  \end{itemize}
\end{itemize}

\subsubsection{Challenges}
\begin{itemize}
  \item How to set the parameters of the method?
  \begin{itemize}
    \item Results depend on proper parameter setting.
  \end{itemize}

  \item How to efficiently retrieve the neighborhood of an object?
  \begin{itemize}
    \item Without suitable index support, complexity is $O(n)$
  \end{itemize}

  \item How to efficiently score only the top outliers?
  \begin{itemize}
    \item Reduce the time required for the k-nearest neighbor distance computations by ruling out objects quickly that are obviously non-outliers (even with approximate computation).
  \end{itemize}

  \item How to deal with locally different densities?
  \begin{itemize}
    \item The nearest neighbor distance of many objects in the sparser cluster $C_1$ is at least as large as the nearest neighbor distance of outlier $p_2$.
  \end{itemize}
\end{itemize}

\newpage

\subsection{Density-based methods}
\subsubsection{Local Outlier Factor}
\begin{itemize}
  \item True distance of objects should be computed in a normalized way, relative to its local distance distribution.
  \item Inspiration from OPTICS algorithm
  \item Reachability distance of object $p$ relative to objective $o$
  $$R_k(p, o) = max\{dis(p, o), knndistance(o)\}$$
  \item If $o$ in dense region and the distance between $o$ and $p$ is large, the reachability distance of $p$ to $o$ is equal to the true distance.
  \item If the distance between $p$ and $o$ is small, then the reachability distance is ``smoothed out'' by the k-nearest neighbor distance of $o$.
  \item Average reachability distance of object $p$ to its neighboring objects
  $$AR_k(p) = \frac{\sum_{o \in L_k(p)} R_k(p, o)}{|L_k(p)|}$$
  where $L_k(p)$ contains all objects within kNN distance from $p$
  \item Local Outlier Factor: mean ratio of $AR_k(p)$ and $AR_k(o)$ for $o$ in the neighborhood of $p$.
  $$LOF_k(p) = \frac{\sum_{o \in L_k(p)}AR_k(p) / AR_k(o)}{|L_k(p)|}$$
  \item LOF values for the objects in a cluster are often close to 1 when the cluster objects are homogeneously distributed.
  \item LOF values of outliers will be much higher, the higher the more object deviates from its neighborhood.
  \item How to determine the parameter $k$?
  \begin{itemize}
    \item k too small: not robust enough
    \item k too large: not local enough
  \end{itemize}
  \item If some outlier $p$ is known, choose $k$ that maximizes value of $LOF_k(p)$
  \item LOF detects contextual outliers
\end{itemize}

\newpage

\subsubsection{Kernel Density Estimation}
\begin{itemize}
  \item Non-parametric method, i.e. makes no assumption on the data distribution.
  \item Estimate the density in a neighborhood by the mean influence of all objects $x_i$ on that neighborhood.
  \item Influence measured by kernel function $K_h$.
  \item Density at a given point in the data space is estimated as the sum of the smoothed values of kernel functions:
  $$f(x) = \frac{1}{n} K_h(x-x_i)$$
  \item Kernel functions define the relevant neighborhood of a point in data space.
  \item For most smooth kernel functions, with increasing data set size the estimate converges to the true density,
  \item Gaussian kernel 
  $$K_h(x-x_i) = (\frac{1}{\sqrt{(2\pi)}h})^d e^{-||x-x_i||^2/(2h^2)}$$
  \item Need to decide which kernel function and which kernel width to use.
  \item The density at each object is computed without including the object itself.
  \item Outlier score depends on the density estimate.
  \item Low values of the density indicate greater tendency to b an outlier.
\end{itemize}

\textbf{Discussion}
\begin{itemize}
  \item Kernel density estimation does not require data to follow a know distribution.
  \item Is usually robust to the choice of the kernel width $h$.
  \item Does not work well for high-dimensional data and accuracy of the density estimation degrades with increasing dimensionality.
  \item Does not work well if local density varies greatly and kernel width is global.
\end{itemize}


\end{document}